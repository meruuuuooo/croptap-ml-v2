{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering (Optimized)\n",
        "\n",
        "This notebook creates the training dataset by merging real datasets and using vectorized operations for efficiency:\n",
        "1. **Historical Crop Performance**: Actual yields per crop-province-year\n",
        "2. **Soil Test Data**: Real farmer soil conditions (NPK, pH) aggregated by province\n",
        "3. **Climate Data**: 5-year averages (2020-2024) for temperature, rainfall, humidity\n",
        "\n",
        "The key improvements are:\n",
        "- **Vectorized Feature Extraction**: Replaced slow `iterrows()` loop with fully vectorized operations.\n",
        "- **Efficient Data Merging**: Optimized merging of crop, climate, and soil data.\n",
        "- **Dynamic Suitability Score**: Calculates scores based on the 95th percentile of each crop's yield."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Add app to path to import modules\n",
        "project_root = Path(\"../\").resolve()\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "from app.services.data_loader import DataLoader\n",
        "from app.services.feature_extractor import FeatureExtractor\n",
        "\n",
        "# Initialize services\n",
        "data_loader = DataLoader()\n",
        "data_loader.load_all_data()\n",
        "feature_extractor = FeatureExtractor()\n",
        "\n",
        "print(\"Data loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preprocess historical performance data\n",
        "historical_perf = data_loader.historical_performance\n",
        "historical_perf['yield_per_ha'] = historical_perf['Volume_Production'] / historical_perf['Area_Planted_Harvested']\n",
        "historical_perf_clean = historical_perf[historical_perf['yield_per_ha'].notna() & (historical_perf['yield_per_ha'] != float('inf')) & (historical_perf['yield_per_ha'] > 0)].copy()\n",
        "print(f\"Valid yield records: {len(historical_perf_clean)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge datasets\n",
        "historical_perf_clean['Province_normalized'] = historical_perf_clean['Province'].str.strip().str.title()\n",
        "merged_data = pd.merge(historical_perf_clean, data_loader.climate_averages, left_on='Province_normalized', right_index=True, how='inner')\n",
        "soil_data = pd.read_csv(data_loader.data_dir / \"soil_test_data.csv\")\n",
        "soil_data['province_normalized'] = soil_data['province'].str.strip().str.title()\n",
        "province_soil_agg = soil_data.groupby('province_normalized').agg(\n",
        "    nitrogen=('nitrogen', lambda x: x.mode()[0] if not x.mode().empty else 'Medium'),\n",
        "    phosphorus=('phosphorus', lambda x: x.mode()[0] if not x.mode().empty else 'Medium'),\n",
        "    potassium=('potassium', lambda x: x.mode()[0] if not x.mode().empty else 'Medium'),\n",
        "    ph_min=('ph_min', 'mean'),\n",
        "    ph_max=('ph_max', 'mean')\n",
        ").reset_index()\n",
        "merged_data = pd.merge(merged_data, province_soil_agg, left_on='Province_normalized', right_on='province_normalized', how='inner')\n",
        "print(f\"Total merged records: {len(merged_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vectorized feature extraction\n",
        "print(\"Extracting features using vectorized operations...\")\n",
        "feature_df = feature_extractor.extract_features_vectorized(merged_data, data_loader.unified_crop_db)\n",
        "print(f\"Feature extraction complete. {len(feature_df)} records created.\")\n",
        "\n",
        "# Calculate dynamic suitability score\n",
        "max_yield_per_crop = historical_perf_clean.groupby('Crop')['yield_per_ha'].quantile(0.95)\n",
        "feature_df['max_yield'] = feature_df['crop_name'].map(max_yield_per_crop).fillna(20)\n",
        "feature_df['suitability_score'] = np.minimum(100.0, np.maximum(0.0, (feature_df['actual_yield'] / feature_df['max_yield']) * 80.0 + 20.0))\n",
        "feature_df = feature_df.drop(columns=['max_yield'])\n",
        "print(\"Suitability scores calculated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and validation sets\n",
        "train_set, val_set = train_test_split(feature_df, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set shape: {train_set.shape}\")\n",
        "print(f\"Validation set shape: {val_set.shape}\")\n",
        "\n",
        "# Save the datasets to a temporary directory\n",
        "output_dir = Path(\"/tmp/models\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "train_set.to_csv(output_dir / \"training_dataset.csv\", index=False)\n",
        "val_set.to_csv(output_dir / \"validation_dataset.csv\", index=False)\n",
        "\n",
        "print(f\"\\nTraining and validation datasets saved to {output_dir}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

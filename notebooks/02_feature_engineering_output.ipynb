{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a774a2a",
   "metadata": {
    "papermill": {
     "duration": 0.003656,
     "end_time": "2025-11-17T19:17:54.434581",
     "exception": false,
     "start_time": "2025-11-17T19:17:54.430925",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering (Optimized)\n",
    "\n",
    "This notebook creates the training dataset by merging real datasets and using vectorized operations for efficiency:\n",
    "1. **Historical Crop Performance**: Actual yields per crop-province-year\n",
    "2. **Soil Test Data**: Real farmer soil conditions (NPK, pH) aggregated by province\n",
    "3. **Climate Data**: 5-year averages (2020-2024) for temperature, rainfall, humidity\n",
    "\n",
    "The key improvements are:\n",
    "- **Vectorized Feature Extraction**: Replaced slow `iterrows()` loop with fully vectorized operations.\n",
    "- **Efficient Data Merging**: Optimized merging of crop, climate, and soil data.\n",
    "- **Dynamic Suitability Score**: Calculates scores based on the 95th percentile of each crop's yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0cbebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T19:17:54.444197Z",
     "iopub.status.busy": "2025-11-17T19:17:54.443404Z",
     "iopub.status.idle": "2025-11-17T19:17:56.732878Z",
     "shell.execute_reply": "2025-11-17T19:17:56.730980Z"
    },
    "papermill": {
     "duration": 2.297334,
     "end_time": "2025-11-17T19:17:56.734926",
     "exception": false,
     "start_time": "2025-11-17T19:17:54.437592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded. Creating unified database and climate averages...\n",
      "Pre-calculated climate averages for 80 provinces.\n",
      "Initialization complete.\n",
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add app to path to import modules\n",
    "project_root = Path(\"../\").resolve()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from app.services.data_loader import DataLoader\n",
    "from app.services.feature_extractor import FeatureExtractor\n",
    "\n",
    "# Initialize services\n",
    "data_loader = DataLoader()\n",
    "data_loader.load_all_data()\n",
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "print(\"Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5d17ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T19:17:56.742188Z",
     "iopub.status.busy": "2025-11-17T19:17:56.741735Z",
     "iopub.status.idle": "2025-11-17T19:17:56.758031Z",
     "shell.execute_reply": "2025-11-17T19:17:56.756057Z"
    },
    "papermill": {
     "duration": 0.021861,
     "end_time": "2025-11-17T19:17:56.759388",
     "exception": false,
     "start_time": "2025-11-17T19:17:56.737527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid yield records: 93369\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess historical performance data\n",
    "historical_perf = data_loader.historical_performance\n",
    "historical_perf['yield_per_ha'] = historical_perf['Volume_Production'] / historical_perf['Area_Planted_Harvested']\n",
    "historical_perf_clean = historical_perf[historical_perf['yield_per_ha'].notna() & (historical_perf['yield_per_ha'] != float('inf')) & (historical_perf['yield_per_ha'] > 0)].copy()\n",
    "print(f\"Valid yield records: {len(historical_perf_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "709761f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T19:17:56.766076Z",
     "iopub.status.busy": "2025-11-17T19:17:56.765645Z",
     "iopub.status.idle": "2025-11-17T19:17:57.031289Z",
     "shell.execute_reply": "2025-11-17T19:17:57.028336Z"
    },
    "papermill": {
     "duration": 0.270823,
     "end_time": "2025-11-17T19:17:57.032688",
     "exception": false,
     "start_time": "2025-11-17T19:17:56.761865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total merged records: 65487\n"
     ]
    }
   ],
   "source": [
    "# Merge datasets\n",
    "historical_perf_clean['Province_normalized'] = historical_perf_clean['Province'].str.strip().str.title()\n",
    "merged_data = pd.merge(historical_perf_clean, data_loader.climate_averages, left_on='Province_normalized', right_index=True, how='inner')\n",
    "soil_data = pd.read_csv(data_loader.data_dir / \"soil_test_data.csv\")\n",
    "soil_data['province_normalized'] = soil_data['province'].str.strip().str.title()\n",
    "province_soil_agg = soil_data.groupby('province_normalized').agg(\n",
    "    nitrogen=('nitrogen', lambda x: x.mode()[0] if not x.mode().empty else 'Medium'),\n",
    "    phosphorus=('phosphorus', lambda x: x.mode()[0] if not x.mode().empty else 'Medium'),\n",
    "    potassium=('potassium', lambda x: x.mode()[0] if not x.mode().empty else 'Medium'),\n",
    "    ph_min=('ph_min', 'mean'),\n",
    "    ph_max=('ph_max', 'mean')\n",
    ").reset_index()\n",
    "merged_data = pd.merge(merged_data, province_soil_agg, left_on='Province_normalized', right_on='province_normalized', how='inner')\n",
    "print(f\"Total merged records: {len(merged_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26037b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T19:17:57.039706Z",
     "iopub.status.busy": "2025-11-17T19:17:57.039266Z",
     "iopub.status.idle": "2025-11-17T19:17:59.884490Z",
     "shell.execute_reply": "2025-11-17T19:17:59.881451Z"
    },
    "papermill": {
     "duration": 2.851245,
     "end_time": "2025-11-17T19:17:59.886539",
     "exception": false,
     "start_time": "2025-11-17T19:17:57.035294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features using vectorized operations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete. 65487 records created.\n",
      "Suitability scores calculated.\n"
     ]
    }
   ],
   "source": [
    "# Vectorized feature extraction\n",
    "print(\"Extracting features using vectorized operations...\")\n",
    "feature_df = feature_extractor.extract_features_vectorized(merged_data, data_loader.unified_crop_db)\n",
    "print(f\"Feature extraction complete. {len(feature_df)} records created.\")\n",
    "\n",
    "# Calculate dynamic suitability score\n",
    "max_yield_per_crop = historical_perf_clean.groupby('Crop')['yield_per_ha'].quantile(0.95)\n",
    "feature_df['max_yield'] = feature_df['crop_name'].map(max_yield_per_crop).fillna(20)\n",
    "feature_df['suitability_score'] = np.minimum(100.0, np.maximum(0.0, (feature_df['actual_yield'] / feature_df['max_yield']) * 80.0 + 20.0))\n",
    "feature_df = feature_df.drop(columns=['max_yield'])\n",
    "print(\"Suitability scores calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc514f63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T19:17:59.895844Z",
     "iopub.status.busy": "2025-11-17T19:17:59.895185Z",
     "iopub.status.idle": "2025-11-17T19:18:01.673954Z",
     "shell.execute_reply": "2025-11-17T19:18:01.671517Z"
    },
    "papermill": {
     "duration": 1.785771,
     "end_time": "2025-11-17T19:18:01.675868",
     "exception": false,
     "start_time": "2025-11-17T19:17:59.890097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (52389, 14)\n",
      "Validation set shape: (13098, 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and validation datasets saved to /tmp/models\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and validation sets\n",
    "train_set, val_set = train_test_split(feature_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {train_set.shape}\")\n",
    "print(f\"Validation set shape: {val_set.shape}\")\n",
    "\n",
    "# Save the datasets to a temporary directory\n",
    "output_dir = Path(\"/tmp/models\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "train_set.to_csv(output_dir / \"training_dataset.csv\", index=False)\n",
    "val_set.to_csv(output_dir / \"validation_dataset.csv\", index=False)\n",
    "\n",
    "print(f\"\\nTraining and validation datasets saved to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.7109,
   "end_time": "2025-11-17T19:18:02.408793",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/02_feature_engineering.ipynb",
   "output_path": "notebooks/02_feature_engineering_output.ipynb",
   "parameters": {},
   "start_time": "2025-11-17T19:17:52.697893",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
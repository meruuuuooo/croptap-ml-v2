{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67d9094f",
   "metadata": {
    "papermill": {
     "duration": 0.002629,
     "end_time": "2025-11-17T16:17:40.264744",
     "exception": false,
     "start_time": "2025-11-17T16:17:40.262115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering (Optimized)\n",
    "\n",
    "This notebook creates the training dataset by merging real datasets and using vectorized operations for efficiency:\n",
    "1. **Historical Crop Performance**: Actual yields per crop-province-year\n",
    "2. **Soil Test Data**: Real farmer soil conditions (NPK, pH) aggregated by province\n",
    "3. **Climate Data**: 5-year averages (2020-2024) for temperature, rainfall, humidity\n",
    "\n",
    "The key improvements are:\n",
    "- **Vectorized Feature Extraction**: Replaced slow `iterrows()` loop with fully vectorized operations.\n",
    "- **Efficient Data Merging**: Optimized merging of crop, climate, and soil data.\n",
    "- **Dynamic Suitability Score**: Calculates scores based on the 95th percentile of each crop's yield."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4f15f24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T16:17:40.271092Z",
     "iopub.status.busy": "2025-11-17T16:17:40.270723Z",
     "iopub.status.idle": "2025-11-17T16:17:42.407157Z",
     "shell.execute_reply": "2025-11-17T16:17:42.405327Z"
    },
    "papermill": {
     "duration": 2.141543,
     "end_time": "2025-11-17T16:17:42.408573",
     "exception": false,
     "start_time": "2025-11-17T16:17:40.267030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded. Creating unified database and climate averages...\n",
      "Pre-calculated climate averages for 80 provinces.\n",
      "Initialization complete.\n",
      "Data loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add app to path to import modules\n",
    "project_root = Path(\"../\").resolve()\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from app.services.data_loader import DataLoader\n",
    "from app.services.feature_extractor import FeatureExtractor\n",
    "from app.utils.data_processor import parse_soil_types\n",
    "\n",
    "# Initialize services, ensuring correct data path\n",
    "data_dir = Path.cwd() / 'raw_datasets'\n",
    "if not data_dir.exists():\n",
    "    data_dir = Path('../raw_datasets').resolve()\n",
    "\n",
    "data_loader = DataLoader(data_dir=str(data_dir))\n",
    "data_loader.load_all_data()\n",
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "print(\"Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b105c4b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T16:17:42.417501Z",
     "iopub.status.busy": "2025-11-17T16:17:42.417098Z",
     "iopub.status.idle": "2025-11-17T16:17:42.435057Z",
     "shell.execute_reply": "2025-11-17T16:17:42.433265Z"
    },
    "papermill": {
     "duration": 0.023782,
     "end_time": "2025-11-17T16:17:42.436940",
     "exception": false,
     "start_time": "2025-11-17T16:17:42.413158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid yield records: 93369\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess historical performance data\n",
    "historical_perf = data_loader.historical_performance\n",
    "historical_perf['yield_per_ha'] = historical_perf['Volume_Production'] / historical_perf['Area_Planted_Harvested']\n",
    "historical_perf_clean = historical_perf[historical_perf['yield_per_ha'].notna() & (historical_perf['yield_per_ha'] != float('inf')) & (historical_perf['yield_per_ha'] > 0)].copy()\n",
    "print(f\"Valid yield records: {len(historical_perf_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02170081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T16:17:42.444319Z",
     "iopub.status.busy": "2025-11-17T16:17:42.443926Z",
     "iopub.status.idle": "2025-11-17T16:17:42.710578Z",
     "shell.execute_reply": "2025-11-17T16:17:42.708867Z"
    },
    "papermill": {
     "duration": 0.272082,
     "end_time": "2025-11-17T16:17:42.711785",
     "exception": false,
     "start_time": "2025-11-17T16:17:42.439703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total merged records: 65487\n"
     ]
    }
   ],
   "source": [
    "# Merge datasets\n",
    "historical_perf_clean['Province_normalized'] = historical_perf_clean['Province'].str.strip().str.title()\n",
    "merged_data = pd.merge(historical_perf_clean, data_loader.climate_averages, left_on='Province_normalized', right_index=True, how='inner')\n",
    "soil_data = pd.read_csv(data_loader.data_dir / \"soil_test_data.csv\")\n",
    "soil_data['province_normalized'] = soil_data['province'].str.strip().str.title()\n",
    "province_soil_agg = soil_data.groupby('province_normalized').agg(\n",
    "    nitrogen=('nitrogen', lambda x: x.mode()[0] if not x.mode().empty else 'Medium'),\n",
    "    phosphorus=('phosphorus', lambda x: x.mode()[0] if not x.mode().empty else 'Medium'),\n",
    "    potassium=('potassium', lambda x: x.mode()[0] if not x.mode().empty else 'Medium'),\n",
    "    ph_min=('ph_min', 'mean'),\n",
    "    ph_max=('ph_max', 'mean')\n",
    ").reset_index()\n",
    "merged_data = pd.merge(merged_data, province_soil_agg, left_on='Province_normalized', right_on='province_normalized', how='inner')\n",
    "print(f\"Total merged records: {len(merged_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d76744a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T16:17:42.721622Z",
     "iopub.status.busy": "2025-11-17T16:17:42.721212Z",
     "iopub.status.idle": "2025-11-17T16:17:44.821767Z",
     "shell.execute_reply": "2025-11-17T16:17:44.820090Z"
    },
    "papermill": {
     "duration": 2.106949,
     "end_time": "2025-11-17T16:17:44.823208",
     "exception": false,
     "start_time": "2025-11-17T16:17:42.716259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features using vectorized operations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction complete. 65487 records created.\n",
      "Suitability scores calculated.\n"
     ]
    }
   ],
   "source": [
    "# Vectorized feature extraction\n",
    "print(\"Extracting features using vectorized operations...\")\n",
    "feature_df = feature_extractor.extract_features_vectorized(merged_data, data_loader.unified_crop_db)\n",
    "print(f\"Feature extraction complete. {len(feature_df)} records created.\")\n",
    "\n",
    "# Calculate dynamic suitability score\n",
    "max_yield_per_crop = historical_perf_clean.groupby('Crop')['yield_per_ha'].quantile(0.95)\n",
    "feature_df['max_yield'] = feature_df['crop_name'].map(max_yield_per_crop).fillna(20)\n",
    "feature_df['suitability_score'] = np.minimum(100.0, np.maximum(0.0, (feature_df['actual_yield'] / feature_df['max_yield']) * 80.0 + 20.0))\n",
    "feature_df = feature_df.drop(columns=['max_yield'])\n",
    "print(\"Suitability scores calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7135f0e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T16:17:44.833173Z",
     "iopub.status.busy": "2025-11-17T16:17:44.832812Z",
     "iopub.status.idle": "2025-11-17T16:17:46.230328Z",
     "shell.execute_reply": "2025-11-17T16:17:46.228638Z"
    },
    "papermill": {
     "duration": 1.405084,
     "end_time": "2025-11-17T16:17:46.231688",
     "exception": false,
     "start_time": "2025-11-17T16:17:44.826604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (52389, 14)\n",
      "Validation set shape: (13098, 14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and validation datasets saved to /tmp/models\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and validation sets\n",
    "train_set, val_set = train_test_split(feature_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {train_set.shape}\")\n",
    "print(f\"Validation set shape: {val_set.shape}\")\n",
    "\n",
    "# Save the datasets to a temporary directory\n",
    "output_dir = Path(\"/tmp/models\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "train_set.to_csv(output_dir / \"training_dataset.csv\", index=False)\n",
    "val_set.to_csv(output_dir / \"validation_dataset.csv\", index=False)\n",
    "\n",
    "print(f\"\\nTraining and validation datasets saved to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.288381,
   "end_time": "2025-11-17T16:17:46.757097",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/02_feature_engineering.ipynb",
   "output_path": "notebooks/02_feature_engineering_optimized.ipynb",
   "parameters": {},
   "start_time": "2025-11-17T16:17:38.468716",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}